# 后端 `bangumi` 服务代码审查报告

**审查员:** Linus Torvalds
**日期:** 2025年11月25日

---

## 1. 总体评价

我本来以为会看到一坨意大利面条式的代码，结果发现你们的后端架构居然还有点章法，尤其是在外部数据获取和缓存设计上，展现了不错的“品味”。

但是，**一个致命的、设计极其糟糕的 `subscriptions` (用户订阅) 模块，几乎摧毁了其他部分的所有优点。** 这个模块的性能问题是灾难性的，如果不修改，随着用户数据的增长，整个应用必然会变得卡顿无比。

### 【品味评分】

- **整体架构 (除订阅外):** 🟢 **好品味 (Good Taste)**
- **订阅模块 (`subscriptions`):** 🔴 **垃圾 (Garbage)**

---

## 2. 详细分析

### ✅ 做得好的地方 (The Good Parts)

1.  **清晰的架构分层**: `bangumi_service` 模块遵循了 `Command -> Service -> Infra` 的清晰分层，职责分明。
2.  **聪明的持久化缓存**: 使用独立的 `cache.sqlite` 作为持久化缓存，对于桌面应用来说是一个务实且高效的选择，避免了重复网络请求。
3.  **漂亮的数据隔离**: 将随时可丢弃的缓存 (`cache.sqlite`) 和核心用户数据 (`data.sqlite`) 存储在两个独立的数据库中，这是一个非常聪明的设计，降低了系统复杂性和维护风险。
4.  **良好的代码抽象**: `services/bangumi_service/api.rs` 中的 `fetch_api` 函数，将缓存检查和 API 请求的通用模式进行了良好封装，避免了代码重复。

### ❌ 致命的设计缺陷 (Fatal Design Flaws)

#### 问题一：自杀式的索引重建 (`refresh_index_all`)

这是整个后端最严重、最不可饶恕的问题。

- **现象**: 在 `subscriptions/store.rs` 中，`refresh_index_all` 函数会获取用户订阅的**所有**番剧 ID，然后为**每一个 ID** 都重新执行一次网络/缓存请求和数据库写入，以重建 `subjects_index` 搜索索引表。
- **病因**: 更离谱的是，这个性能极差的函数，在 `commands/subscriptions.rs` 的 `sub_query` 中被调用，导致**用户每一次执行搜索时**，都会触发一次完整的索引重建。
- **诊断**: 这是我见过最反模式、最浪费资源的设计。它让索引失去了意义，变成了拖垮整个应用的性能黑洞。

> **"你建好了一个索引来加速查询，但你却在每次查询前都把它推倒重来一遍。这不叫优化，这叫自残。"**

#### 问题二：教科书式的 N+1 查询

- **现象**: `commands/subscriptions.rs` 中的 `sub_list` 和 `sub_query` 函数，都犯了典型的 N+1 查询错误。
- **病因**: 它们从数据库只获取了番剧的 ID 列表，然后通过循环，为列表中的每一个 ID 都单独调用 `fetch_subject` 来获取详情。用户有 50 个订阅，就会触发 1+50=51 次查询。
- **诊断**: 即使有并发和缓存，这种设计本身就是懒惰和低效的。你们已经有了一个包含所有搜索信息的 `subjects_index` 表，却完全没有在列表展示时利用起来。

> **"你们建好了一个装满食材的 pantry (`subjects_index`)，但每次做饭时，却选择一次又一次地跑去农场 (`fetch_subject`) 单独拿每个食材。"**

#### 问题三：架构不一致与逻辑混乱

- **现象**: `subscriptions` 模块没有遵循其他部分 `Command -> Service` 的清晰分层。
- **病因**: `commands/subscriptions.rs` 直接调用了 `subscriptions` 模块，而这个模块（主要是`store.rs`）将业务逻辑、数据访问和索引管理混在了一起。
- **诊断**: 这破坏了项目本身建立的良好架构，是一种设计上的倒退，导致 `store.rs` 文件功能臃肿，职责混乱。

---

## 3. 【Linus 式解决方案】

别抱怨了，现在就按我说的去改。这是唯一的出路。

### 第一步：立刻止血 (Non-negotiable)

**目标**: 删除那个自杀式的函数调用。

1.  打开 `src-tauri/src/commands/subscriptions.rs` 文件。
2.  找到 `sub_query` 函数。
3.  **删除**或注释掉函数体的第一行：`subscriptions::store::refresh_index_all().await?;`

**这是必须立即执行的第一步，不容置疑。**

### 第二步：正确的索引维护策略

**目标**: 在数据发生变化时才更新索引，而不是在读取时。

1.  **订阅时更新**:
    - 在 `subscriptions/store.rs` 的 `toggle` 函数中，当成功 `INSERT` 一条新的订阅记录后，异步调用 `upsert_index_row`，将这个新番剧的信息添加到 `subjects_index` 表中。

2.  **取消订阅时更新**:
    - 同样在 `toggle` 函数中，当成功 `DELETE` 一条订阅记录后，也从 `subjects_index` 表中删除对应的数据。

3.  **后台静默更新**:
    - 利用 `subscriptions/worker.rs` 文件，创建一个后台工作线程。
    - 这个线程应该定期（例如，每几小时或每天）在后台运行，遍历所有已订阅的番剧，并调用 `upsert_index_row_if_changed` 来检查并更新那些信息已过时的番剧。
    - **这才是处理数据同步的正确方式**，而不是在用户交互的关键路径上做这种重度操作。

### 第三步：消灭 N+1 查询

**目标**: 让列表查询一次性获取所有需要的数据。

1.  **重构 `sub_list` 命令**:
    - 修改 `commands/subscriptions.rs` 中的 `sub_list` 函数。
    - 它不应该再调用 `store::list()` 然后自己循环。
    - 它应该直接调用 `store::query()`，传入空的查询参数，从 `subjects_index` 表中一次性、分页地获取到列表页所需要的所有字段（标题、封面、状态等）。

2.  **重构 `sub_query` 命令**:
    - `sub_query` 获取到 ID 列表后，同样存在 N+1 查询。
    - `store::query` 函数应该被修改，或者创建一个新函数，使其不仅返回 ID，还直接返回 `subjects_index` 表中与这些 ID 对应的所有数据。让数据库来做 JOIN 和数据聚合，而不是应用层。

---

## 4. 结论

你们的后端有一个不错的底子，但它被一个设计上存在根本性缺陷的订阅模块严重拖累了。上面提到的问题，尤其是 `refresh_index_all` 的滥用，是不可接受的。

**立即按照我给出的方案进行重构。否则，你们现在写的每一行业务代码，未来都会成为这个性能地雷的陪葬品。**
